---
layout: page
title: "Something About Me"
description: "Hey there, welcome to my page！"
header-img: "img/post-bg-rwd.jpg"
---




<div class="zh post-container">

    <blockquote>
        <p> Snow nothing,<br>

            reap nothing.</p>
    </blockquote>

    <p>Hey，my name is <strong>Shuai Gao (Leo).</strong>

        I am a formal mapping engineer, current Master of Data Science graduate from RMIT.

        I am familiar with Python, Excel, R, Java and SQL, skilful in time series analysis,
        data mining, machine learning and statistics. I am also acclimated to big data and cloud computing technologies.

        e.g. Google cloud, MapReduce, Hadoop. <br>
        <br>
        <a href="./CV/english.pdf">Download my CV here</a><br>
        <a href="/CV/chinese.pdf">Download Chinese CV</a><br>

        <hr size=3 />
        <h3>Education</h3>
        <div>

            <a href="https://www.rmit.edu.au/"><U><b>Royal Melbourne Institute of Technology<br></U></a>
            <I> MSc Data Science<br></b></I>
            Melbourne, Australia<br>
            Jun 2017 - Jun 2019<br>
            <p>
                <a href="http://www.hebut.edu.cn/"><U> <b>City University of Hebei University of Technology <br></U></a>
                <I> BSc Surveying and Mapping Engineering<br></b></I>
                Tianjin, China<br>
                Sep 2012 – Jul 2016
            </p>
        </div>


        <hr size=3 width=1000 />
        <h3>SKILLS AND CERTIFICATE</h3>
        <br>
        •<b>Certified machine learning analyst by ITman</b><br>
        <br>
        •<b>Language:</b> <br>English (Sophisticated), Mandarin (MT)<br>
        <br>
        •<b>Software</b>:<br> Proficient in <b>Excel</b> and <b>Python</b>; familiar with <b>SQL</b>, <b>JAVA</b> and <b>R</b>; familiar with <b>SAS</b><br>
        <br>
        •<b>Keywords</b>: <br>Supervised | Unsupervised | Semisupervised | Reinforcement Learning | MapReduce | Time Series Analysis |Regression | SVM | Random Forest | Feature Selection | Dimensionality reduction | Data Collection | Scrappy |
        Artificial Neural Network | Lasso Regression | Ridge Regression | Data Pre-processing | Cloud computing |
        Logistic Regression | K Means | DB SCAN | Crawl |
        <br>



        <hr size=3 width=1000 />
        <h3>Working Experience</h3>

        <a href="https://www.colesgroup.com.au/home/"><U> <b>Coles </b></U></a><br>
        Coles Supermarkets Australia Pty Ltd, trading as Coles, is an Australian supermarket,
        retail and consumer services chain,
        headquartered in Melbourne as part of the Coles Group. <br>
        <I><b>Advanced Analytics Internship<br></b></I>
        &nbsp;&nbsp;• Used unsupervised machine learning algorithms for user information grouping based on the purchase behavior and personal information statistics of users in Australia; <br>
        &nbsp;&nbsp;• Responsible for data processing, including extracting data from SQL, cleaning data, feature selection and statistical analysis as well as data visualization; <br>
        &nbsp;&nbsp;• Results: Scored the user value according to the user's RFM (recent purchase, purchase frequency, purchase expenses), which was recognized by the team. Grouped the information such as discount tendency, quality requirement,
        purchase quantity and prices, etc., laying the basis for advertisement placement;
        <br>
        Melbourne, Australia<br>
        MAR 2019 – Present <br><br>

        <a href="https://itman.design/"><U> <b>IT MAN</b></U></a><br>
        Leading IT Consulting Agency in Melbourne. Affordable IT professionals for crafting amazing websites,
        mobile apps. We are also experts in digital marketing and providing excellent IT solutions
        to help the business grow faster<br>
        <I><b>Data Scientist Intern<br></b></I>
        &nbsp;&nbsp;• Independently researched the Beijing job market and used machine learning algorithms for salary forecasting; <br>
        &nbsp;&nbsp;• Grabbed job related data from 51Job, performing statistical analysis and feature selection for data cleaning and processing;<br>
        &nbsp;&nbsp;• Used deep learning methods (Random Forest, SVM, etc.) to build a machine learning model to predict job salary levels; <br>
        &nbsp;&nbsp;• Results: The salary level can be basically predicted given a description of the position information, and the prediction accuracy reached over 80%;
        <br>
        Melbourne, Australia<br>
        JUN 2018 – FEB 2019 <br><br>






        <hr size=3 width=1000 />
        <h3>Project</h3>
        • <a href="http://leoshuaigao.com/2019/01/17/machine-learning/">End to end data analysis of regional job market</a>: <br>
        Data collection, data pre-processing, words cloud, modelling and association finding. Analyze the information about the job market in Beijing, China.
        <br><br>
        • <a href="https://github.com/UltramanShuai/scrapy_foundamental">Scrapy project</a>: <br>
        Crawling data from popular websites.
        <br><br>
        • <a href="http://leoshuaigao.com/2019/01/15/wechat_bot/">Wechat Robot</a>: <br>
        By using Mini Batch K-Means algorithm to covert picture received from WeChat to reduce the size of the picture without distortion, and send back picture to the
        friend.<br><br>
        • <a href="https://github.com/UltramanShuai/request_basic">Web analysis based on Request package</a>: <br>
        Using Request package to get the information from HTML sit.<br><br>
        • <a href="http://leoshuaigao.com/2019/01/16/shinny_work/">Dashboard</a>: <br>
        visualize the gender diversity analysis of Australia labour force<br><br>
        • <a href="http://leoshuaigao.com/2019/01/17/gdp/">Interactive visualization</a>: <br>
        Interactive visualization of the relationship between population life expectancy and the gross domestic product has changed over time.<br><br>
        • <a href="http://leoshuaigao.com/2019/03/01/web/">Cloud-based application</a>: <br>
        Web application for analysis health indicator of every country in serval years. Using serval web tech to collect, analysis data and track users' action.<br><br>



</div>
